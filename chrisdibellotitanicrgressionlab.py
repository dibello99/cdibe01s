
# coding: utf-8

# In[ ]:




# In[ ]:

#CSC570R-Chris DiBello Titanic Regression Lab
import pandas as pd
get_ipython().magic(u'pylab inline')
#reading the data from the disk into memory
df = pd.read_csv("train.csv")
#Just a reminder, here are all the column names
df.columns
#I'm going to create a new dataframe and put only the three variables I'm going to be using into it.
X = pd.DataFrame()
#average = df.Sex.mean()
#df.Sex=df.Sex.fillna(value=average)
#average2 = df.Embarked.mean()
#df.Embarked=df.Embarked.fillna(value=average2)
X['sex'] = df['Sex']
#average = df.Sex.mean()
#df.Sex=df.Sex.fillna(value=average)
X['embarked'] = df['Embarked']
X['fare'] = df['Fare']
X['survived'] = df['Survived']
average = df.Age.mean()
df.Age=df.Age.fillna(value=average)
X['age'] = df['Age']
#I'm going to drop missing values.   That's probably NOT the best strategy, but it's usually good to start simple and 
#build complexity as you go.
X = X.dropna(axis=0)
#survived will be my dependent variable, y.   I'll assign it to y and remove it from X
y = X['survived']
X = X.drop(['survived'], axis=1)
# We need to handle Sex such that it's categorical, for logistic regression.
# Currently it's a string
#refer back to last week's lecture if you forget why we're doing this

#We can use pandas get_dummies to implement one hot encoding.
pd.get_dummies(X.sex)
pd.get_dummies(X.embarked)
#IMPORTANT! get_dummies returns an indicator variable for each category.
#Refering back to my talk on encoding variables, it's important to drop one category
#Otherwise you'll have two perfectly colinear variables.   

#Here, since I only have two variables it's easy, I'll just take one, and reassign it to sex
#so now Sex becomes female = 1, male = 0
X['sex'] = pd.get_dummies(X.sex)['male']
X['embarked'] = pd.get_dummies(X.embarked)['C']
#remember to scale our features, as with linear regression
from sklearn.cross_validation import train_test_split
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X= scaler.fit_transform(X)
#build test and training sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
#This function looks for females in the test set and returns 1, survived, otherwise it returns 0
def base_rate_model(X):
    y = np.zeros(X.shape[0])
  
    return y
    
#how accurate is my base rate model?
y_base_rate = base_rate_model(X_test)
#print 'xtest'
#print X_test
from sklearn.metrics import accuracy_score
print "Base rate accuracy is %2.2f" % accuracy_score(y_test, y_base_rate)

from sklearn.linear_model import LogisticRegression
model = LogisticRegression(penalty='l2', C=1)
model.fit(X_train, y_train)
print "Logistic accuracy is %2.2f" % accuracy_score(y_test,model.predict(X_test))
from sklearn.metrics import roc_auc_score
from sklearn.metrics import classification_report

print "---Base Model---"
#base rate AUC
base_roc_auc = roc_auc_score(y_test, base_rate_model(X_test))
print "Base Rate AUC = %2.2f" % base_roc_auc
print classification_report(y_test,base_rate_model(X_test) )
#print "\n\n---Logistic Model---"
#logistic AUC
logit_roc_auc = roc_auc_score(y_test, model.predict(X_test))
print "Logistic AUC = %2.2f" % logit_roc_auc
print classification_report(y_test, model.predict(X_test) )
from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])
# Plot of a ROC curve for a specific class
get_ipython().magic(u'pylab inline')
get_ipython().magic(u'matplotlib inline')
plt.figure()
plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic example')
plt.legend(loc="lower right")
plt.show()


# In[ ]: